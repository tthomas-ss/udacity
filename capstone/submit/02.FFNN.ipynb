{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "1bfa1b01eb23052d0ebd4a432397e5f7d867a4a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import re\n",
    "import string\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNLSTM, CuDNNGRU, Conv1D, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, roc_curve, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split, KFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "from capstone_utils import *\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('max_colwidth', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "1d0f6fca4b1c84e383dd4a82da2f18bdd41accfc"
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ef82d6ee6721cf9608e8b1cca35917bdd8f9f927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading questions...\n",
      "Done loading train - Loading test\n",
      "Done loading test\n"
     ]
    }
   ],
   "source": [
    "train, test, corpus = load_data('../data', clean=False, lower_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "9221d5a8721b1655abe18f1996020c4445929a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Word: . - could not convert string to float: '.'\n",
      "Word: at - could not convert string to float: 'name@domain.com'\n",
      "Word: . - could not convert string to float: '.'\n",
      "Word: to - could not convert string to float: 'name@domain.com'\n",
      "Word: . - could not convert string to float: '.'\n",
      "Word: . - could not convert string to float: '.'\n",
      "Word: email - could not convert string to float: 'name@domain.com'\n",
      "Word: or - could not convert string to float: 'name@domain.com'\n",
      "Word: contact - could not convert string to float: 'name@domain.com'\n",
      "Word: Email - could not convert string to float: 'name@domain.com'\n",
      "Word: on - could not convert string to float: 'name@domain.com'\n",
      "Word: At - could not convert string to float: 'Killerseats.com'\n",
      "Word: by - could not convert string to float: 'name@domain.com'\n",
      "Word: in - could not convert string to float: 'mylot.com'\n",
      "Word: emailing - could not convert string to float: 'name@domain.com'\n",
      "Word: Contact - could not convert string to float: 'name@domain.com'\n",
      "Word: at - could not convert string to float: 'name@domain.com'\n",
      "Word: â€¢ - could not convert string to float: 'name@domain.com'\n",
      "Word: at - could not convert string to float: 'Amazon.com'\n",
      "Word: is - could not convert string to float: 'name@domain.com'\n",
      "Found 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word2vec = load_embeddings(path='../data/glove.840B.300d/glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "ae6dfa003b86ad242352a7268f6e92d48834e8d1"
   },
   "outputs": [],
   "source": [
    "# convert the sentences (strings) into integers\n",
    "targets = train['target'].values\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(train[\"question_text\"])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "81c7685e889db9c83432222e5953e6c1949585cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 227538 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = load_embedding_matrix(word2idx, word2vec, MAX_VOCAB_SIZE, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "730b93ffd8927e4485ad45c9066930ef25b1e8fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1306122, 150)\n"
     ]
    }
   ],
   "source": [
    "# pad sequences so that we get a N x T matrix\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "21336f9908a9aa203f92f2a4c5580f8537126e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of testdata tensor: (56370, 150)\n"
     ]
    }
   ],
   "source": [
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of testdata tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "582520344658023de69ce859f28d83081e0cf577"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "83bb6288a76aa89a5cafc73aea1483af7bd746f0"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "  MAX_VOCAB_SIZE,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=MAX_SEQUENCE_LENGTH,\n",
    "  trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "ba670f548f8bb2055fd2f0994ef4db5e73d64ee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 300)          9000000   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 9,118,273\n",
      "Trainable params: 118,273\n",
      "Non-trainable params: 9,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_cp_filepath = 'ffnn_embeddings.ep-{epoch:02d}-loss-{val_loss:.2f}.hdf5'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'],\n",
    "  \n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "f352df6abd4f9efae805e8cd9f6bd7cdd53c1636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 10 epochs\n",
      "Train on 914285 samples, validate on 391837 samples\n",
      "Epoch 1/10\n",
      "914285/914285 [==============================] - 15s 17us/step - loss: 0.1551 - acc: 0.9434 - val_loss: 0.1389 - val_acc: 0.9476\n",
      "Epoch 2/10\n",
      "914285/914285 [==============================] - 15s 16us/step - loss: 0.1439 - acc: 0.9456 - val_loss: 0.1353 - val_acc: 0.9486\n",
      "Epoch 3/10\n",
      "914285/914285 [==============================] - 15s 16us/step - loss: 0.1406 - acc: 0.9466 - val_loss: 0.1346 - val_acc: 0.9481\n",
      "Epoch 4/10\n",
      "914285/914285 [==============================] - 15s 16us/step - loss: 0.1386 - acc: 0.9470 - val_loss: 0.1345 - val_acc: 0.9496\n",
      "Epoch 5/10\n",
      "914285/914285 [==============================] - 15s 16us/step - loss: 0.1368 - acc: 0.9476 - val_loss: 0.1335 - val_acc: 0.9478\n",
      "Epoch 6/10\n",
      "914285/914285 [==============================] - 15s 16us/step - loss: 0.1362 - acc: 0.9479 - val_loss: 0.1301 - val_acc: 0.9495\n",
      "Epoch 7/10\n",
      "914285/914285 [==============================] - 15s 16us/step - loss: 0.1354 - acc: 0.9479 - val_loss: 0.1282 - val_acc: 0.9503\n",
      "Epoch 8/10\n",
      "914285/914285 [==============================] - 15s 16us/step - loss: 0.1345 - acc: 0.9483 - val_loss: 0.1316 - val_acc: 0.9486\n",
      "Epoch 9/10\n",
      "914285/914285 [==============================] - 15s 16us/step - loss: 0.1337 - acc: 0.9484 - val_loss: 0.1303 - val_acc: 0.9494\n",
      "Epoch 10/10\n",
      "914285/914285 [==============================] - 15s 16us/step - loss: 0.1333 - acc: 0.9484 - val_loss: 0.1287 - val_acc: 0.9495\n"
     ]
    }
   ],
   "source": [
    "print('Training model for {} epochs'.format(EPOCHS))\n",
    "\n",
    "model_cp = ModelCheckpoint(model_cp_filepath, monitor='val_loss', verbose=0, \n",
    "                     save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "r = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                   validation_data=(X_test, y_test), callbacks=[model_cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "958cfb0288494fe6b09c6267b92109344bae8d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.49348509809794816\n",
      "F1 score at threshold 0.11 is 0.5070646299696578\n",
      "F1 score at threshold 0.12 is 0.5169442227302858\n",
      "F1 score at threshold 0.13 is 0.5271973558904263\n",
      "F1 score at threshold 0.14 is 0.536399021835072\n",
      "F1 score at threshold 0.15 is 0.5428311645724336\n",
      "F1 score at threshold 0.16 is 0.5490635210381801\n",
      "F1 score at threshold 0.17 is 0.5553189811529059\n",
      "F1 score at threshold 0.18 is 0.5602955234037038\n",
      "F1 score at threshold 0.19 is 0.5653917181027168\n",
      "F1 score at threshold 0.2 is 0.569245234635439\n",
      "F1 score at threshold 0.21 is 0.5725433678401322\n",
      "F1 score at threshold 0.22 is 0.5744827087252207\n",
      "F1 score at threshold 0.23 is 0.5752611365702518\n",
      "F1 score at threshold 0.24 is 0.5752481649108128\n",
      "F1 score at threshold 0.25 is 0.5755709720165251\n",
      "F1 score at threshold 0.26 is 0.5742601954163086\n",
      "F1 score at threshold 0.27 is 0.5722757295053473\n",
      "F1 score at threshold 0.28 is 0.5695906432748539\n",
      "F1 score at threshold 0.29 is 0.5662414131501472\n",
      "F1 score at threshold 0.3 is 0.563005251345521\n",
      "F1 score at threshold 0.31 is 0.5580827151170539\n",
      "F1 score at threshold 0.32 is 0.5529510160717919\n",
      "F1 score at threshold 0.33 is 0.5483069562016931\n",
      "F1 score at threshold 0.34 is 0.542049519534264\n",
      "F1 score at threshold 0.35 is 0.5368658666983486\n",
      "F1 score at threshold 0.36 is 0.5308507812311334\n",
      "F1 score at threshold 0.37 is 0.5253158358632848\n",
      "F1 score at threshold 0.38 is 0.5190877664204531\n",
      "F1 score at threshold 0.39 is 0.511421287765971\n",
      "F1 score at threshold 0.4 is 0.5041021433699109\n",
      "F1 score at threshold 0.41 is 0.4973335761296533\n",
      "F1 score at threshold 0.42 is 0.48923978770035115\n",
      "F1 score at threshold 0.43 is 0.4807253802612871\n",
      "F1 score at threshold 0.44 is 0.4718281092562839\n",
      "F1 score at threshold 0.45 is 0.46398585947856824\n",
      "F1 score at threshold 0.46 is 0.455215308323761\n",
      "F1 score at threshold 0.47 is 0.4443749466920649\n",
      "F1 score at threshold 0.48 is 0.43427846202659437\n",
      "F1 score at threshold 0.49 is 0.42280896902991627\n",
      "F1 score at threshold 0.5 is 0.4121946871100018\n",
      "F1 score at threshold 0.51 is 0.3998431041245512\n",
      "F1 score at threshold 0.52 is 0.38733797076578924\n",
      "F1 score at threshold 0.53 is 0.37436216552582446\n",
      "F1 score at threshold 0.54 is 0.36014284359890025\n",
      "F1 score at threshold 0.55 is 0.3445553451651209\n",
      "F1 score at threshold 0.56 is 0.3293657263981738\n",
      "F1 score at threshold 0.57 is 0.31248135256091497\n",
      "F1 score at threshold 0.58 is 0.29687973341411694\n",
      "F1 score at threshold 0.59 is 0.2805649601586813\n",
      "F1 score at threshold 0.6 is 0.26154969235582437\n",
      "F1 score at threshold 0.61 is 0.24371291325233116\n",
      "F1 score at threshold 0.62 is 0.22398563734290844\n",
      "F1 score at threshold 0.63 is 0.20348879643821619\n",
      "F1 score at threshold 0.64 is 0.1825523245045379\n",
      "F1 score at threshold 0.65 is 0.16188755926845788\n",
      "F1 score at threshold 0.66 is 0.1425080189399725\n",
      "F1 score at threshold 0.67 is 0.12462770278110857\n",
      "F1 score at threshold 0.68 is 0.10439625083336601\n",
      "F1 score at threshold 0.69 is 0.08955815335843074\n",
      "F1 score at threshold 0.7 is 0.07473793710490517\n",
      "F1 score at threshold 0.71 is 0.06079935405732741\n",
      "F1 score at threshold 0.72 is 0.04882216526302942\n",
      "F1 score at threshold 0.73 is 0.03881587028620563\n",
      "F1 score at threshold 0.74 is 0.02989745912778487\n",
      "F1 score at threshold 0.75 is 0.02241892786234282\n",
      "F1 score at threshold 0.76 is 0.015778109948513534\n",
      "F1 score at threshold 0.77 is 0.011902284739273378\n",
      "F1 score at threshold 0.78 is 0.00933605634976868\n",
      "F1 score at threshold 0.79 is 0.007094270333430706\n",
      "F1 score at threshold 0.8 is 0.005181130656415828\n",
      "F1 score at threshold 0.81 is 0.003095586697343652\n",
      "F1 score at threshold 0.82 is 0.0021763696480140626\n",
      "F1 score at threshold 0.83 is 0.0015072221059242205\n",
      "F1 score at threshold 0.84 is 0.0012562288011389808\n",
      "F1 score at threshold 0.85 is 0.0007539582809751194\n",
      "F1 score at threshold 0.86 is 0.00041893590280687055\n",
      "F1 score at threshold 0.87 is 0.0003351627634169844\n",
      "F1 score at threshold 0.88 is 0.0002513826043237808\n",
      "F1 score at threshold 0.89 is 0.0002513826043237808\n",
      "Best F1 0.5755709720165251 at 0.25\n"
     ]
    }
   ],
   "source": [
    "val_predictions = model.predict(X_test)\n",
    "best_threshold = threshold_search(y_test, val_predictions, min_threshold=0.1, max_threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "e284eaa2436f6469dd8a29a84250c27c968ac642"
   },
   "outputs": [],
   "source": [
    "submission_predictions = model.predict(test_data)\n",
    "submission_best_predictions = (submission_predictions > best_threshold.get('threshold')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "9ebe26e48ffed5c03d8d15b53f63b5fbcfce7d66"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"qid\":test[\"qid\"].values})\n",
    "submission['prediction'] = submission_best_predictions\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import capstone_utils\n",
    "importlib.reload(capstone_utils)\n",
    "from capstone_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
