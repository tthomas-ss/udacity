{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "1bfa1b01eb23052d0ebd4a432397e5f7d867a4a3"
   },
   "outputs": [],
   "source": [
    "# All neccessary imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, auc, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from capstone_utils import *\n",
    "# Inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "7ffc9df4605b5b392159f244a3d0d2a354eb7be6"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize input text, including lammatization and stop-word removal. Onlu used for logistic regression\n",
    "\n",
    "    Args:\n",
    "    text (str) -- a string to tokenize\n",
    "\n",
    "    Returns:\n",
    "    tokens (list) -- a list of words tokenized from text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    # lemmatize andremove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "0763ddd4b5956389649df3afdf5fc4a0c8889d1f"
   },
   "outputs": [],
   "source": [
    "def build_model(max_ngram = 1):\n",
    "    \"\"\"\n",
    "    Return a simple pipeline with bag-of-words.\n",
    "    Param: max_ngram - use this to produce n-grams\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([('vect', CountVectorizer(tokenizer=tokenize, ngram_range=(1,max_ngram))),\n",
    "                         ('clf', LogisticRegression(class_weight='balanced'))\n",
    "                         ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4a504fb801452d2fbb80e1b6f4be8d99663ab2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in comments...\n",
      "Cleaning train\n",
      "Cleaning test\n"
     ]
    }
   ],
   "source": [
    "# prepare text samples and their labels\n",
    "path = '../data/'\n",
    "print('Loading in comments...')\n",
    "train = pd.read_csv(path+'train.csv')\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "print('Cleaning train')\n",
    "train['question_text'] = train['question_text'].map(lambda x: clean_text(x))\n",
    "print('Cleaning test')\n",
    "test['question_text'] = test['question_text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "c5fcf0105f542d45a9af95b3fe784e00cba5d2c0"
   },
   "outputs": [],
   "source": [
    "# Split the training data into a train and test (validation) set\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.question_text, train.target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "81c7685e889db9c83432222e5953e6c1949585cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the pipeline and fit model\n",
    "model = build_model()\n",
    "print('Training model...')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "730b93ffd8927e4485ad45c9066930ef25b1e8fb"
   },
   "outputs": [],
   "source": [
    "# Run prediction (yes - we could obviously set the cutoff to 0.5 and get the same result from predict_proba, \n",
    "# but I am lazy..)\n",
    "predictions = model.predict(X_test)\n",
    "predictions_proba = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "21336f9908a9aa203f92f2a4c5580f8537126e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5204351508045078\n"
     ]
    }
   ],
   "source": [
    "# Print the f1-score\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "22641385f8ab22f45c427ce621f7dcf9e9e53707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.2712944884853332\n",
      "F1 score at threshold 0.11 is 0.28311165547506356\n",
      "F1 score at threshold 0.12 is 0.29395433532312154\n",
      "F1 score at threshold 0.13 is 0.3043866309661778\n",
      "F1 score at threshold 0.14 is 0.3144051775290822\n",
      "F1 score at threshold 0.15 is 0.32404850093847454\n",
      "F1 score at threshold 0.16 is 0.33329406220546653\n",
      "F1 score at threshold 0.17 is 0.34199295000568547\n",
      "F1 score at threshold 0.18 is 0.3501580726043824\n",
      "F1 score at threshold 0.19 is 0.35817818617658814\n",
      "F1 score at threshold 0.2 is 0.36625140264229145\n",
      "F1 score at threshold 0.21 is 0.37360867632380207\n",
      "F1 score at threshold 0.22 is 0.38119687473169056\n",
      "F1 score at threshold 0.23 is 0.3878451778208717\n",
      "F1 score at threshold 0.24 is 0.39459047960234656\n",
      "F1 score at threshold 0.25 is 0.40131879372385704\n",
      "F1 score at threshold 0.26 is 0.4076863635940453\n",
      "F1 score at threshold 0.27 is 0.41409265493859365\n",
      "F1 score at threshold 0.28 is 0.42032230049213554\n",
      "F1 score at threshold 0.29 is 0.426101724865751\n",
      "F1 score at threshold 0.3 is 0.4319050995037296\n",
      "F1 score at threshold 0.31 is 0.43717731546755656\n",
      "F1 score at threshold 0.32 is 0.442381595079383\n",
      "F1 score at threshold 0.33 is 0.44744904894856274\n",
      "F1 score at threshold 0.34 is 0.45251337352466675\n",
      "F1 score at threshold 0.35 is 0.4576333247178427\n",
      "F1 score at threshold 0.36 is 0.462284447453777\n",
      "F1 score at threshold 0.37 is 0.46681947535683055\n",
      "F1 score at threshold 0.38 is 0.4712479951547236\n",
      "F1 score at threshold 0.39 is 0.476051820425619\n",
      "F1 score at threshold 0.4 is 0.48065304616519855\n",
      "F1 score at threshold 0.41 is 0.484776258908751\n",
      "F1 score at threshold 0.42 is 0.4888646959979173\n",
      "F1 score at threshold 0.43 is 0.4935441717717862\n",
      "F1 score at threshold 0.44 is 0.4974466588226732\n",
      "F1 score at threshold 0.45 is 0.5015097581932\n",
      "F1 score at threshold 0.46 is 0.5051159787413699\n",
      "F1 score at threshold 0.47 is 0.5086628221074718\n",
      "F1 score at threshold 0.48 is 0.5127193596416436\n",
      "F1 score at threshold 0.49 is 0.5167422213064083\n",
      "F1 score at threshold 0.5 is 0.5204351508045078\n",
      "F1 score at threshold 0.51 is 0.5241013879552374\n",
      "F1 score at threshold 0.52 is 0.5277474065975092\n",
      "F1 score at threshold 0.53 is 0.5307950727883539\n",
      "F1 score at threshold 0.54 is 0.5342206100936822\n",
      "F1 score at threshold 0.55 is 0.5372080840387107\n",
      "F1 score at threshold 0.56 is 0.5399436745392187\n",
      "F1 score at threshold 0.57 is 0.542851506355156\n",
      "F1 score at threshold 0.58 is 0.5456203339602211\n",
      "F1 score at threshold 0.59 is 0.5485088603947558\n",
      "F1 score at threshold 0.6 is 0.5508922718333455\n",
      "F1 score at threshold 0.61 is 0.5533156498673739\n",
      "F1 score at threshold 0.62 is 0.5560143706861853\n",
      "F1 score at threshold 0.63 is 0.558292925338072\n",
      "F1 score at threshold 0.64 is 0.5606148227058286\n",
      "F1 score at threshold 0.65 is 0.5632164453539721\n",
      "F1 score at threshold 0.66 is 0.5652626832719188\n",
      "F1 score at threshold 0.67 is 0.5674095495552133\n",
      "F1 score at threshold 0.68 is 0.5698130588777179\n",
      "F1 score at threshold 0.69 is 0.5712495783335743\n",
      "F1 score at threshold 0.7 is 0.5729122715913147\n",
      "F1 score at threshold 0.71 is 0.5747662018047579\n",
      "F1 score at threshold 0.72 is 0.5770284860061458\n",
      "F1 score at threshold 0.73 is 0.5784476819411537\n",
      "F1 score at threshold 0.74 is 0.5796756536225478\n",
      "F1 score at threshold 0.75 is 0.581294864953263\n",
      "F1 score at threshold 0.76 is 0.5825926313221095\n",
      "F1 score at threshold 0.77 is 0.5832568321446225\n",
      "F1 score at threshold 0.78 is 0.5839614216824434\n",
      "F1 score at threshold 0.79 is 0.5840682324393822\n",
      "F1 score at threshold 0.8 is 0.5848931953314248\n",
      "F1 score at threshold 0.81 is 0.58522706140514\n",
      "F1 score at threshold 0.82 is 0.5845120893214137\n",
      "F1 score at threshold 0.83 is 0.5845977231416818\n",
      "F1 score at threshold 0.84 is 0.584880739647269\n",
      "F1 score at threshold 0.85 is 0.5836242775706648\n",
      "F1 score at threshold 0.86 is 0.5817861363135653\n",
      "F1 score at threshold 0.87 is 0.5799258021117861\n",
      "F1 score at threshold 0.88 is 0.5778625637622858\n",
      "F1 score at threshold 0.89 is 0.5734431575158279\n",
      "F1 score at threshold 0.9 is 0.5689684937419076\n",
      "F1 score at threshold 0.91 is 0.564578567657927\n",
      "F1 score at threshold 0.92 is 0.5594978288748396\n",
      "F1 score at threshold 0.93 is 0.5518418563882546\n",
      "F1 score at threshold 0.94 is 0.5430184025801555\n",
      "F1 score at threshold 0.95 is 0.5315407973356843\n",
      "F1 score at threshold 0.96 is 0.5143364177969324\n",
      "F1 score at threshold 0.97 is 0.4915227296810427\n",
      "F1 score at threshold 0.98 is 0.4571045201688424\n",
      "F1 score at threshold 0.99 is 0.39735901629414255\n",
      "Best F1 0.58522706140514 at 0.81\n"
     ]
    }
   ],
   "source": [
    "# Search for the best threshold\n",
    "best_threshold = threshold_search(y_test, predictions_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "e284eaa2436f6469dd8a29a84250c27c968ac642"
   },
   "outputs": [],
   "source": [
    "# Create predictions on the test-set based on the best threshold found above\n",
    "benchmark_submission_predictions = model.predict(test['question_text'])\n",
    "benchmark_predictions = (benchmark_submission_predictions > best_threshold.get('threshold')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "9ebe26e48ffed5c03d8d15b53f63b5fbcfce7d66"
   },
   "outputs": [],
   "source": [
    "# Create a submission data frame and write to disk\n",
    "submission = pd.DataFrame({\"qid\":test[\"qid\"].values})\n",
    "submission['prediction'] = benchmark_predictions\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading WordNet: Package 'WordNet' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'capstone_utils' from '/home/ubuntu/jupyter_notebooks/data-science/capstone/submit/capstone_utils.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this to avoid restarting the kernel if making changes to capstone_utils.\n",
    "import importlib\n",
    "import capstone_utils\n",
    "importlib.reload(capstone_utils)\n",
    "from capstone_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
