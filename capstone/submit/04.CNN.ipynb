{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "1bfa1b01eb23052d0ebd4a432397e5f7d867a4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading WordNet: Package 'WordNet' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import re\n",
    "import string\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNLSTM, CuDNNGRU, Conv1D, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, roc_curve, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split, KFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "from capstone_utils import *\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('max_colwidth', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "1d0f6fca4b1c84e383dd4a82da2f18bdd41accfc"
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f6131317d805d5745097b79bb2f4401822bb2bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300\tparagram_300_sl999\r\n",
      "glove.840B.300d\t\t\twiki-news-300d-1M\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "ef82d6ee6721cf9608e8b1cca35917bdd8f9f927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading questions...\n",
      "Done loading train - Loading test\n",
      "Done loading test\n"
     ]
    }
   ],
   "source": [
    "train, test, corpus = load_data('../data', clean=False, lower_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9221d5a8721b1655abe18f1996020c4445929a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Word: . - could not convert string to float: '.'\n",
      "Word: at - could not convert string to float: 'name@domain.com'\n",
      "Word: . - could not convert string to float: '.'\n",
      "Word: to - could not convert string to float: 'name@domain.com'\n",
      "Word: . - could not convert string to float: '.'\n",
      "Word: . - could not convert string to float: '.'\n",
      "Word: email - could not convert string to float: 'name@domain.com'\n",
      "Word: or - could not convert string to float: 'name@domain.com'\n",
      "Word: contact - could not convert string to float: 'name@domain.com'\n",
      "Word: Email - could not convert string to float: 'name@domain.com'\n",
      "Word: on - could not convert string to float: 'name@domain.com'\n",
      "Word: At - could not convert string to float: 'Killerseats.com'\n",
      "Word: by - could not convert string to float: 'name@domain.com'\n",
      "Word: in - could not convert string to float: 'mylot.com'\n",
      "Word: emailing - could not convert string to float: 'name@domain.com'\n",
      "Word: Contact - could not convert string to float: 'name@domain.com'\n",
      "Word: at - could not convert string to float: 'name@domain.com'\n",
      "Word: â€¢ - could not convert string to float: 'name@domain.com'\n",
      "Word: at - could not convert string to float: 'Amazon.com'\n",
      "Word: is - could not convert string to float: 'name@domain.com'\n",
      "Found 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word2vec = load_embeddings(path='../data/glove.840B.300d/glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "ae6dfa003b86ad242352a7268f6e92d48834e8d1"
   },
   "outputs": [],
   "source": [
    "# convert the sentences (strings) into integers\n",
    "targets = train['target'].values\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train[\"question_text\"])\n",
    "sequences = tokenizer.texts_to_sequences(train[\"question_text\"])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "81c7685e889db9c83432222e5953e6c1949585cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 222161 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = load_embedding_matrix(word2idx, word2vec, MAX_VOCAB_SIZE, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "730b93ffd8927e4485ad45c9066930ef25b1e8fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1306122, 300)\n"
     ]
    }
   ],
   "source": [
    "# pad sequences so that we get a N x T matrix\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "21336f9908a9aa203f92f2a4c5580f8537126e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of testdata tensor: (56370, 300)\n"
     ]
    }
   ],
   "source": [
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of testdata tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "582520344658023de69ce859f28d83081e0cf577"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "83bb6288a76aa89a5cafc73aea1483af7bd746f0"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "  MAX_VOCAB_SIZE,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=MAX_SEQUENCE_LENGTH,\n",
    "  trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "ba670f548f8bb2055fd2f0994ef4db5e73d64ee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 300)          9000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 298, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 97, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 9,230,529\n",
      "Trainable params: 230,529\n",
      "Non-trainable params: 9,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "f352df6abd4f9efae805e8cd9f6bd7cdd53c1636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 5 epochs\n",
      "Train on 914285 samples, validate on 391837 samples\n",
      "Epoch 1/5\n",
      "914285/914285 [==============================] - 223s 244us/step - loss: 0.1425 - acc: 0.9492 - val_loss: 0.1326 - val_acc: 0.9514\n",
      "Epoch 2/5\n",
      "914285/914285 [==============================] - 217s 238us/step - loss: 0.1317 - acc: 0.9520 - val_loss: 0.1303 - val_acc: 0.9532\n",
      "Epoch 3/5\n",
      "914285/914285 [==============================] - 220s 241us/step - loss: 0.1266 - acc: 0.9535 - val_loss: 0.1295 - val_acc: 0.9531\n",
      "Epoch 4/5\n",
      "914285/914285 [==============================] - 221s 241us/step - loss: 0.1215 - acc: 0.9552 - val_loss: 0.1328 - val_acc: 0.9537\n",
      "Epoch 5/5\n",
      "914285/914285 [==============================] - 219s 240us/step - loss: 0.1164 - acc: 0.9568 - val_loss: 0.1343 - val_acc: 0.9502\n"
     ]
    }
   ],
   "source": [
    "print('Training model for {} epochs'.format(EPOCHS))\n",
    "r = model.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "958cfb0288494fe6b09c6267b92109344bae8d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.46143153331448494\n",
      "F1 score at threshold 0.11 is 0.47325908701505326\n",
      "F1 score at threshold 0.12 is 0.4822246043256196\n",
      "F1 score at threshold 0.13 is 0.4902811411403633\n",
      "F1 score at threshold 0.14 is 0.4973613726166168\n",
      "F1 score at threshold 0.15 is 0.5027133094255363\n",
      "F1 score at threshold 0.16 is 0.5077271221989182\n",
      "F1 score at threshold 0.17 is 0.5131908663456937\n",
      "F1 score at threshold 0.18 is 0.5203693801754411\n",
      "F1 score at threshold 0.19 is 0.531051448061454\n",
      "F1 score at threshold 0.2 is 0.5352112676056339\n",
      "F1 score at threshold 0.21 is 0.5389111426300285\n",
      "F1 score at threshold 0.22 is 0.5422367026744902\n",
      "F1 score at threshold 0.23 is 0.5471798595981603\n",
      "F1 score at threshold 0.24 is 0.55050736497545\n",
      "F1 score at threshold 0.25 is 0.5533735819014131\n",
      "F1 score at threshold 0.26 is 0.5576590330788805\n",
      "F1 score at threshold 0.27 is 0.5597154688063778\n",
      "F1 score at threshold 0.28 is 0.5619319566807117\n",
      "F1 score at threshold 0.29 is 0.5641902010183047\n",
      "F1 score at threshold 0.3 is 0.56652697280428\n",
      "F1 score at threshold 0.31 is 0.5683083743309726\n",
      "F1 score at threshold 0.32 is 0.5698038929795144\n",
      "F1 score at threshold 0.33 is 0.5711179895349694\n",
      "F1 score at threshold 0.34 is 0.5718682556066436\n",
      "F1 score at threshold 0.35 is 0.5729610886829286\n",
      "F1 score at threshold 0.36 is 0.5741557097404838\n",
      "F1 score at threshold 0.37 is 0.5751423811553213\n",
      "F1 score at threshold 0.38 is 0.5758963519649288\n",
      "F1 score at threshold 0.39 is 0.5763066890623456\n",
      "F1 score at threshold 0.4 is 0.5760728478143659\n",
      "F1 score at threshold 0.41 is 0.5764648871299923\n",
      "F1 score at threshold 0.42 is 0.5762898586204787\n",
      "F1 score at threshold 0.43 is 0.575797543483637\n",
      "F1 score at threshold 0.44 is 0.5757304400341531\n",
      "F1 score at threshold 0.45 is 0.5755982672330403\n",
      "F1 score at threshold 0.46 is 0.5750254755434784\n",
      "F1 score at threshold 0.47 is 0.5733930216120119\n",
      "F1 score at threshold 0.48 is 0.5726078312732301\n",
      "F1 score at threshold 0.49 is 0.5716528222905357\n",
      "F1 score at threshold 0.5 is 0.5710324795352522\n",
      "F1 score at threshold 0.51 is 0.5702607845750588\n",
      "F1 score at threshold 0.52 is 0.5686564488661827\n",
      "F1 score at threshold 0.53 is 0.566947639601365\n",
      "F1 score at threshold 0.54 is 0.5660497766432674\n",
      "F1 score at threshold 0.55 is 0.5631425220211127\n",
      "F1 score at threshold 0.56 is 0.5609841011953115\n",
      "F1 score at threshold 0.57 is 0.5590792958769895\n",
      "F1 score at threshold 0.58 is 0.5573165539102048\n",
      "F1 score at threshold 0.59 is 0.5534639273769708\n",
      "F1 score at threshold 0.6 is 0.5496098374121229\n",
      "F1 score at threshold 0.61 is 0.5453879583964061\n",
      "F1 score at threshold 0.62 is 0.5418096531292433\n",
      "F1 score at threshold 0.63 is 0.5367378582503877\n",
      "F1 score at threshold 0.64 is 0.528996546821044\n",
      "F1 score at threshold 0.65 is 0.5214339174645779\n",
      "F1 score at threshold 0.66 is 0.5136339800734138\n",
      "F1 score at threshold 0.67 is 0.5049626467449306\n",
      "F1 score at threshold 0.68 is 0.4928474299700843\n",
      "F1 score at threshold 0.69 is 0.48195305580159437\n",
      "F1 score at threshold 0.7 is 0.4701103055265607\n",
      "F1 score at threshold 0.71 is 0.4571395762760522\n",
      "F1 score at threshold 0.72 is 0.44438605710282014\n",
      "F1 score at threshold 0.73 is 0.42987043860691787\n",
      "F1 score at threshold 0.74 is 0.41787979462398067\n",
      "F1 score at threshold 0.75 is 0.40382134299932415\n",
      "F1 score at threshold 0.76 is 0.389712537844502\n",
      "F1 score at threshold 0.77 is 0.3749762070934586\n",
      "F1 score at threshold 0.78 is 0.3587594300083823\n",
      "F1 score at threshold 0.79 is 0.3441656292996134\n",
      "F1 score at threshold 0.8 is 0.32825797872340423\n",
      "F1 score at threshold 0.81 is 0.3094400865566676\n",
      "F1 score at threshold 0.82 is 0.2916509255761239\n",
      "F1 score at threshold 0.83 is 0.27251474231480516\n",
      "F1 score at threshold 0.84 is 0.25276752767527677\n",
      "F1 score at threshold 0.85 is 0.2319676860934795\n",
      "F1 score at threshold 0.86 is 0.20998606935992375\n",
      "F1 score at threshold 0.87 is 0.1872321395296836\n",
      "F1 score at threshold 0.88 is 0.16329468818670909\n",
      "F1 score at threshold 0.89 is 0.14282419863171653\n",
      "F1 score at threshold 0.9 is 0.12093095785739347\n",
      "F1 score at threshold 0.91 is 0.09857317892573415\n",
      "F1 score at threshold 0.92 is 0.07869981185701132\n",
      "F1 score at threshold 0.93 is 0.06287884910692638\n",
      "F1 score at threshold 0.94 is 0.04492362982929021\n",
      "F1 score at threshold 0.95 is 0.03386765310316482\n",
      "F1 score at threshold 0.96 is 0.022918131799942083\n",
      "F1 score at threshold 0.97 is 0.011409535706849886\n",
      "F1 score at threshold 0.98 is 0.004597701149425286\n",
      "F1 score at threshold 0.99 is 0.0006702412868632708\n",
      "Best F1 0.5764648871299923 at 0.41\n"
     ]
    }
   ],
   "source": [
    "val_predictions = model.predict(X_test)\n",
    "best_threshold = threshold_search(y_test, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "e284eaa2436f6469dd8a29a84250c27c968ac642"
   },
   "outputs": [],
   "source": [
    "submission_predictions = model.predict(test_data)\n",
    "submission_best_predictions = (cnn_submission_predictions > best_threshold.get('threshold')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "9ebe26e48ffed5c03d8d15b53f63b5fbcfce7d66"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"qid\":test[\"qid\"].values})\n",
    "submission['prediction'] = submission_best_predictions\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import capstone_utils\n",
    "importlib.reload(capstone_utils)\n",
    "from capstone_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
